import configparser
import os
import time
from datetime import datetime, timedelta

from airflow import DAG
from airflow.models import DAG
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.python_operator import (BranchPythonOperator,
                                               PythonOperator)
from airflow.operators.subdag_operator import SubDagOperator

from db_cmd import (Stock2, check_stock_day_exist, db_connect, insert_new_data,
                    save_stock_to_disk, stock2df, update_stock_info)

CONFIG = configparser.ConfigParser()
STOCK_HOME = os.environ['STOCK_HOME'] + '/'
CONFIG.read(STOCK_HOME + 'config.ini')
DB_PATH = STOCK_HOME + str(CONFIG['COMMON']['DB_PATH'])
STOCK_CODE = 'SHOULD_BE_STOCK_CODE'
MAIN_DAG = str(CONFIG['AIRFLOW']['MAIN_DAG']).format(STOCK_CODE)


default_args = {
    'owner': 'user',
    'retries': 1,
    'retry_delay': timedelta(seconds=10)
}


def get_stock_list(**context):

    connection = db_connect(DB_PATH)
    cursor = connection.cursor()

    query_sql = """
                SELECT first_record_date
                FROM stock
                WHERE stock_code = ?"""
    cursor.execute(query_sql, (STOCK_CODE, ))
    dag_start_date = cursor.fetchone()
    if not dag_start_date:
        exit
    dag_start_date = datetime.strptime(dag_start_date[0], "%Y-%m-%d")
    connection.close()

    return dag_start_date


def day_check(stock_code, db_path, **context):
    execution_date = context['execution_date']

    is_existed = check_stock_day_exist(stock_code, execution_date, db_path)

    if is_existed:
        print('[day_check]', stock_code, execution_date, '[skip]')
        return 'done'
    else:
        print('[day_check]', stock_code, execution_date, '[fetch]')
        return 'fetch_stock'


def fetch_stock(stock_code, **context):
    """ Get month data"""
    execution_date = context['execution_date']
    stock = Stock2(str(stock_code))
    stock.fetch(execution_date.year, execution_date.month)
    updated_df = stock2df(stock)

    print("[***************]%s" % stock_code, updated_df.shape)
    return updated_df


def update_db(stock_code, db_path, **context):
    """ """
    updated_df = context['task_instance'].xcom_pull(task_ids='fetch_stock')

    insert_new_data(int(stock_code), updated_df, db_path)

    print('[*******update_db********]', updated_df.shape)


def save2disk(stock_code, **context):
    """ Save fetched data to local disk """
    updated_df = context['task_instance'].xcom_pull(task_ids='fetch_stock')
    execution_date = context['execution_date']
    year = execution_date.year

    save_stock_to_disk(int(stock_code), updated_df, year)

    print('[*******save2disk********]', updated_df.shape)


dag_start_date = get_stock_list()
dag = DAG(
    dag_id=MAIN_DAG,
    default_args=default_args,
    start_date=dag_start_date,
    schedule_interval=timedelta(days=1))

done = DummyOperator(
    task_id='done',
    dag=dag,
)

day_check_operator = BranchPythonOperator(
    task_id='day_check',
    python_callable=day_check,
    provide_context=True,
    op_args=[STOCK_CODE, DB_PATH],
    dag=dag,
)

fetch_stock_operator = PythonOperator(
    task_id='fetch_stock',
    python_callable=fetch_stock,
    op_args=[STOCK_CODE],
    provide_context=True,
    retries=5,
    retry_delay=timedelta(minutes=1),
    dag=dag,
)

update_db_operator = PythonOperator(
    task_id='update_db',
    python_callable=update_db,
    op_args=[STOCK_CODE, DB_PATH],
    provide_context=True,
    dag=dag,
)

save2disk_operator = PythonOperator(
    task_id='save2disk',
    python_callable=save2disk,
    op_args=[STOCK_CODE],
    provide_context=True,
    dag=dag,
)

update_stock_info_operator = PythonOperator(
    task_id='update_stock_info',
    python_callable=update_stock_info,
    op_args=[STOCK_CODE, DB_PATH],
    dag=dag,
)

day_check_operator >> fetch_stock_operator >> [update_db_operator, save2disk_operator] >> update_stock_info_operator
day_check_operator >> done
